Cattura con parametri (fps)
Time-stamping
Storicizzazione in coda
posizione ip o seriale
in seguito alla storicizzazione, analisi del video

priorità informazione (questione ultimi fotogrammi/ scartare quelli vecchi)
ragionare sui requisiti in funzione dei test, anche automatizzati

Il seguente lavoro di tesi si colloca nell’ambito di un’attività di tirocinio che consiste nella partecipazione a un progetto di sviluppo software nell'ambito dei servizi internet e per sistemi che operano nel campo dei servizi marittimi.

La problematica che viene affrontata nella durata dell’attività di tirocinio riguarda l’acquisizione di dati provenienti da IP Camera/WebCam e annessi metadati, la loro storicizzazione e successivamente, la possibilità di poterne effettuare la visualizzazione e la ricerca.

La cattura verrà effettuata utilizzando la libreria open source OpenCV.
Le immagini verranno poi arricchite dei metadati quali la marcatura temporale relativa all’ istante di acquisizione e la posizione geografica prelevate da sistema GPS e/o la rete dati.
I dati verranno poi inseriti in una coda, andando ad affrontare il problema “produttore/consumatore” : la coda verrà realizzata con un in-memory database (scelta determinata dalle esigenze in merito alla performance della soluzione) tramite Redis.
In seguito alla storicizzazione verrà composto il video grazie ai frame e gli annessi metadati precedentemente trattati.
Il video potrà poi essere analizzato in base alle necessità e i diversi parametri.


SCELTA THREADING LIBRARY PYTHON
What's the difference between Python threading and multiprocessing?
With threading, concurrency is achieved using multiple threads, but due to the GIL only one thread can be running at a time. In multiprocessing, the original process is forked process into multiple child processes bypassing the GIL. Each child process will have a copy of the entire program's memory.

Both multithreading and multiprocessing allow Python code to run concurrently. Only multiprocessing will allow your code to be truly parallel. However, if your code is IO-heavy (like HTTP requests), then multithreading will still probably speed up your code.

you can use threading if your program is network bound or multiprocessing if it's CPU bound.

Threading is game-changing because many scripts related to network/data I/O spend the majority of their time waiting for data from a remote source. Because downloads might not be linked (i.e., scraping separate websites), the processor can download from different data sources in parallel and combine the result at the end. For CPU intensive processes, there is little benefit to using the threading module.

What Is Multiprocessing? How Is It Different Than Threading?
Without multiprocessing, Python programs have trouble maxing out your system's specs because of the GIL (Global Interpreter Lock). Python wasn't designed considering that personal computers might have more than one core (shows you how old the language is), so the GIL is necessary because Python is not thread-safe and there is a globally enforced lock when accessing a Python object. Though not perfect, it's a pretty effective mechanism for memory management. What can we do?
Multiprocessing allows you to create programs that can run concurrently (bypassing the GIL) and use the entirety of your CPU core. Though it is fundamentally different from the threading library, the syntax is quite similar. The multiprocessing library gives each process its own Python interpreter and each their own GIL.
Because of this, the usual problems associated with threading (such as data corruption and deadlocks) are no longer an issue. Since the processes don't share memory, they can't modify the same memory concurrently.

What Should You Use?
If your code has a lot of I/O or Network usage:
Multithreading is your best bet because of its low overhead
If you have a GUI
Multithreading so your UI thread doesn't get locked up
If your code is CPU bound:
You should use multiprocessing (if your machine has multiple cores)

The threading module uses threads, the multiprocessing module uses processes. The difference is that threads run in the same memory space, while processes have separate memory. This makes it a bit harder to share objects between processes with multiprocessing. Since threads use the same memory, precautions have to be taken or two threads will write to the same memory at the same time. This is what the global interpreter lock is for.
Spawning processes is a bit slower than spawning threads.

Multiprocessing
Pros
Separate memory space
Code is usually straightforward
Takes advantage of multiple CPUs & cores
Avoids GIL limitations for cPython
Eliminates most needs for synchronization primitives unless if you use shared memory (instead, it's more of a communication model for IPC)
Child processes are interruptible/killable
Python multiprocessing module includes useful abstractions with an interface much like threading.Thread
A must with cPython for CPU-bound processing
Cons
IPC a little more complicated with more overhead (communication model vs. shared memory/objects)
Larger memory footprint
Threading
Pros
Lightweight - low memory footprint
Shared memory - makes access to state from another context easier
Allows you to easily make responsive UIs
cPython C extension modules that properly release the GIL will run in parallel
Great option for I/O-bound applications
Cons
cPython - subject to the GIL
Not interruptible/killable
If not following a command queue/message pump model (using the Queue module), then manual use of synchronization primitives become a necessity (decisions are needed for the granularity of locking)
Code is usually harder to understand and to get right - the potential for race conditions increases dramatically
nice comando x aumentare priorità







produttore inserisce dati + metadati in coda (singola x tutti i produttori)--->
---> coda li passa al consumatore che li salva in locale

il numero di consumatori inizialmente è proporzionale ai produttori ma potrebbe esserne distaccato


i dati passati alla coda sono : frame - timestamp - posizione

il consumatore salva questi dati andando a creare cartelle che precisano il mese / giorno / ora / minuto e così via per essere più facilmente ricercabili

passo il frame ottenuto dalla cattura così com'è e lo faccio salvare in locale
direttamente dal consumatore, in modo da ridurre il lavoro del produttore che lo sta catturando
al frame aggiungo i metadati che mi servono e li passo alla queue

il consumatore legge il pacchetto di dati, legge i metadati, controlla se esiste il path:
se esiste salva il pacchetto dati, altrimenti crea il path e poi lo salva
- i consumatori fanno infinitamente un check se la lista è vuota, quando non lo è prendono il dato
    (qui c'è da provare se effettivamente c'è bisogno o meno di un semaforo per put/get)

valkka library


#TODO
-----NUMERO DI CONSUMATORI PARAMETRICO
-----GRAB E RETRIEVE, VALUTARE PRESTAZIONI ---> grab e retrieve perde molti frames
-----PROVARE A RIDURRE RITARDO INIZIALE CON UNA READ PRIMA DEL WHILE  --> inutile

-------------------------
discover dei dispositivi compatibili con onvif, invece di prendere la roba dal JSON
---CREAZIONE AUTOMATICA FOLDERS


monitor del file system che controlla lo stato dei file e updata di conseguenza il db

TO DO
--use cases
---chiamate differenziate (detail qui https://blog.logrocket.com/django-rest-framework-create-api/)
---come fare upload immagini / come formattarle
---questione database ---> sqlite dovrebbe andar bene, devo capire come formattare 
---delete (guarda sotto)
---rifinire get single frame e documentare utilizzo
        (fare funzione per formattare dati from input ---> generare chiamata api formattata come serve)
---gestire push to api/api storage dal programma principale(es: tenere 1 fps x invio)
-refactoring    -api
                -programma principale

-aggiustare single frame, magari con una query più specifica
-TASK CHE DATO UN INTERVALLO DI TEMPO NE CREA IL VIDEO
-REFACTORING - UNIT TESTING
-TEST DI INTEGRAZIONE

DELETE HOW TO
processo in background che sarà schedulato per essere runnato giornalmente e che cancella sia la foto sia la entry nel db
-api call per la entry nel db sicuramente
--- da vedere se basta questa per eliminare anche la foto in sè
------ altrimenti il processo fa la api call e magari prende i nomi / path e cancella anche quelli in locale


GET SINGLE FRAME

MODALITA' DI PARSING
jsondata = r.json()
print(jsondata)
lista = json.loads(jsondata)
for j in lista:
    for i in j:
      pdata = Post_data(i["path"], i["frame"], i["timestamp"],i["position"],i["name"])
      path = pdata.frame
      img = cv2.imread(path[1:], 1)
      cv2.imshow('frame', img)
      cv2.waitKey(0)
      cv2.destroyWindow('frame')

FORMAT DATA
data1='2022/05/30 12:34:00'
data2='2022/05/30 12:34:29'








USE CASES
-prendi tutti (inutile)
-prendi in base alla camera --- un giorno  -> intervallo di tempo, più generico
                            --- un' ora
                            --- un minuto
-prendi in base ad una posizione --- in un intervallo di tempo
-prendere un frame significativo al secondo, in un intervallo di tempo ---
        --- come sceglierlo? - analisi video
                             - uno con criterio fisso (primo / ultimo / ...)


VIDEO CREATOR
per prendere e utilizzare i files da richiesta get_camera_frames

lista = json.loads(r.content)
for i in lista:
  print(i['frame'])


ffmpeg -y -r 1/5 -f concat -safe 0 -i "output.txt" -c:v libx264 -vf "fps=25,format=yuv420p" "out.mp4"
ffmpeg -y -r 1/5 -f concat -safe 0 -i "output.txt" -c:v libx264 -vf "fps=2,format=yuv420p" "out.mp4"

CREA IL FILE OUTPUT.TXT DA DARE IN INPUT A FFMPEG
lista = json.loads(r.content)
f = open('output.txt', 'w+')
for i in lista:
  path = i['path']
  frame = i['frame']
  fullpath = path+frame
  #print(fullpath)
  f.write("file '"+fullpath+"' \n")
f.close()


mi servirebbe il nome del file singolo da salvare in db 

-serve capire lo stato di 